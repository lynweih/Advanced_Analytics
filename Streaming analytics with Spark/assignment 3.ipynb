{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bded7e83-dea1-416e-88c8-27bf23393ed6",
   "metadata": {},
   "source": [
    "<h1>Assignment 3: Streaming Analytics on Text data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42117be8-91ff-44b9-9101-8b9d74df0d39",
   "metadata": {},
   "source": [
    "<h2> 3.1 Data Collection</h2>\n",
    "In this assignment, we construct a predictive model on Spark streaming by using the text data from the website \"hacker news\". We set the trigger time as 30 seconds, to make sure the data collection and lower latency. After collecting the text on the streaming for ten days, 4887 news are collected. 4698 news are finally taken into account after checking the duplication. The columns as below are set as the schema at first for the dataframe to read the one-piece json data. The column \"aid\" is used to check the duplication and other possible check. Other features are planned to be used as the features. The \"frontpage\" column will be marked as label in the final predictive model, with 3947 false and 751 true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "467aa1d0-c7f7-49b4-b126-960f1c8078be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ec2f70-f68b-4dfd-9221-a78170b622f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 23:16:59 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- aid: string (nullable = false)\n",
      " |-- title: string (nullable = false)\n",
      " |-- url: string (nullable = false)\n",
      " |-- domain: string (nullable = false)\n",
      " |-- votes: string (nullable = false)\n",
      " |-- user: string (nullable = false)\n",
      " |-- posted_at: string (nullable = false)\n",
      " |-- comments: string (nullable = false)\n",
      " |-- source_title: string (nullable = false)\n",
      " |-- source_text: string (nullable = false)\n",
      " |-- frontpage: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+------------+-----+----------+-------------------+--------+--------------------+--------------------+---------+\n",
      "|     aid|               title|                 url|      domain|votes|      user|          posted_at|comments|        source_title|         source_text|frontpage|\n",
      "+--------+--------------------+--------------------+------------+-----+----------+-------------------+--------+--------------------+--------------------+---------+\n",
      "|40177878|Show HN: Rule Spe...|https://gashlin.n...| gashlin.net|    1|       hcs|2024-04-27 06:52:49|       0|                NULL|ðŸ‘| ðŸ‘  \\n---|---...|    false|\n",
      "|40168625|Bitcoin Will Powe...|https://www.coind...|coindesk.com|    1| PaulHoule|2024-04-26 12:33:29|       0|Bitcoin Will Powe...|Bitcoin Will Powe...|    false|\n",
      "|40164119|Sen. Lummis: It'l...|https://www.coind...|coindesk.com|    2| PaulHoule|2024-04-25 23:01:23|       0|Sen. Lummis: It'l...|U.S. Sen. Lummis:...|    false|\n",
      "|40205353|In the AI Economy...|https://reason.co...|  reason.com|    5|   jazzdev|2024-04-29 23:20:52|       3|In the AI Economy...|Will Artificial I...|     true|\n",
      "|40205033|Wasabi Wallet-Dev...|https://www.coind...|coindesk.com|    1|paulpauper|2024-04-29 22:46:57|       0|Wasabi Wallet-Dev...|zkSNACKS, Develop...|    false|\n",
      "+--------+--------------------+--------------------+------------+-----+----------+-------------------+--------+--------------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HistoricalData\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "columns_schema = StructType([\n",
    "    StructField('aid', StringType(), nullable=False),\n",
    "    StructField('title', StringType(), nullable=False),\n",
    "    StructField('url', StringType(), nullable=False),\n",
    "    StructField('domain', StringType(), nullable=False),\n",
    "    StructField('votes', StringType(), nullable=False),\n",
    "    StructField('user', StringType(), nullable=False),\n",
    "    StructField('posted_at', StringType(), nullable=False),\n",
    "    StructField('comments', StringType(), nullable=False),\n",
    "    StructField('source_title', StringType(), nullable=False),\n",
    "    StructField('source_text', StringType(), nullable=False),\n",
    "    StructField('frontpage', StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Create an empty DataFrame with the defined schema\n",
    "empty_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema=columns_schema)\n",
    "empty_df.printSchema()\n",
    "\n",
    "# Read JSON data and provide the schema\n",
    "data = spark.read.format(\"json\").schema(columns_schema).load(\"notebooks/saved_stories/*\")\n",
    "\n",
    "df = empty_df.union(data)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41e311c7-0f4f-4adf-91b1-e91766692861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4887"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95b33d0-ecf4-452d-988c-1d8a6a0ccdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4698"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=df.dropDuplicates([\"aid\"])\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b275551e-4546-4972-a9e6-72de3810f94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.write.parquet(\"hacker_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d046f4b0-52fb-47aa-9344-e6e046e914fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:====================================================> (150 + 2) / 153]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|frontpage|count|\n",
      "+---------+-----+\n",
      "|    false| 3947|\n",
      "|     true|  751|\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.groupBy(\"frontpage\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633bb05b-faad-4b28-a093-73ad5cdcfdfa",
   "metadata": {},
   "source": [
    "<h2> 3.2 Data Preprocessing</h2>\n",
    "After checking the types of columns, the \"comments\" and \"votes\" are transformed to numeric columns. The \"post_time\" could also influence the time when the readers browse it. We assume that in the midnight, the readers may be less active than in the morning. Therefore, the hour is extracted to evaluate the moment for the readers. Missing value is based on the \"source_text\" column where we will implement the text propress. The \"domain\" column as planned will be used to be a categorical feature to predict the model, but the result shows various results, finally we remove it for our final analysis. Considering the outline and the structure of the website of \"hacker news\", we extract 5 features to implement our models: \"title\", \"votes\",\"comments\",\"source_text\",\"hour\"(extracted from the \"post_at\" time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "640467c7-d1df-41e2-8237-d747ccfee145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType,FloatType,IntegerType\n",
    "from pyspark.ml.feature import Tokenizer, StringIndexer, VectorAssembler,StopWordsRemover, Word2Vec\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier,RandomForestClassifier,GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import hour, to_timestamp,col, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43de8ba7-5359-48bb-9e1e-cc9c132010e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.parquet(\"hacker_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46be87dc-f292-4f32-b28f-ee75f748d33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+-----+----+---------+--------+------------+-----------+---------+------------+----+\n",
      "|aid|title|url|domain|votes|user|posted_at|comments|source_title|source_text|frontpage|posted_at_ts|hour|\n",
      "+---+-----+---+------+-----+----+---------+--------+------------+-----------+---------+------------+----+\n",
      "|  0|    0|  0|     0|    0|   0|        0|       0|         141|         28|        0|           0|   0|\n",
      "+---+-----+---+------+-----+----+---------+--------+------------+-----------+---------+------------+----+\n",
      "\n",
      "+--------------------+-----+\n",
      "|              domain|count|\n",
      "+--------------------+-----+\n",
      "| zzbbyy.substack.com|    1|\n",
      "|      zukunftsme.com|    2|\n",
      "|            zork.net|    1|\n",
      "|      zooniverse.org|    1|\n",
      "| zoedolan.medium.com|    1|\n",
      "|      zmescience.com|    1|\n",
      "|         zkpaper.com|    1|\n",
      "|          zilliz.com|    1|\n",
      "|      zhangluyao.com|    1|\n",
      "|           zeteo.com|    1|\n",
      "|zephyrtronium.git...|    1|\n",
      "| zeonic-republic.net|    1|\n",
      "|        zenstack.dev|    1|\n",
      "|            zenoh.io|    1|\n",
      "|       zenhabits.net|    1|\n",
      "|        zeitvice.com|    2|\n",
      "|             zed.dev|    2|\n",
      "|           zdnet.com|    7|\n",
      "|       zapenergy.com|    1|\n",
      "|zaidesanton.subst...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#change the type for numeric features\n",
    "df=df.withColumn(\"votes\", col(\"votes\").cast(IntegerType()))\n",
    "df=df.withColumn(\"comments\", col(\"comments\").cast(IntegerType()))\n",
    "# change the date to the hours\n",
    "df1 = df.withColumn(\"posted_at_ts\", to_timestamp(df[\"posted_at\"], \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df1 = df1.withColumn(\"hour\", hour(\"posted_at_ts\"))\n",
    "#check missing value\n",
    "na=df1.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df1.columns])\n",
    "na.show()\n",
    "## drop it the train-test\n",
    "#check if we can categorize the domain\n",
    "df1.groupBy(\"domain\").count().orderBy(col(\"domain\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2332e101-3ad2-43b4-ad6d-4755d559384c",
   "metadata": {},
   "source": [
    "In order to process the text, we at first to remove the news where there is no source text. Next, we label the frontpage as \"1\" on the frontpage and \"0\" not on the frontpage. In order to train the model, the data is split to train and test datas using the 80-20 approach in a random way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bb2ff34-bd3d-462d-bbe8-e204416e332d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#drop the missing value and add the label to the frontpage\n",
    "df1=df1.dropna(subset=[\"source_text\"])\n",
    "frontpage=StringIndexer(inputCol=\"frontpage\",outputCol=\"label\")\n",
    "df1=frontpage.fit(df1).transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7efd48d0-7a9c-4e8c-85d9-91dc5ee99822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the train-test dataset\n",
    "(train, test)=df1.randomSplit([0.8,0.2],seed=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf1d30-e7df-4909-a81d-c95b4bf9e9c4",
   "metadata": {},
   "source": [
    "<h2> 3.3 Text Propressing </h2> \n",
    "The processing of the text is applied to the columns of \"title\" and \"source_text\". We use the Spark Ml.lib mostly to deal with the text. First we tokenize the title and the source text to get the words, then we remove the stop words to clear out the non-informative words, avoiding the redundancy. Then tf-idf and the word2vector two ways are taken into consideration to transform the text data. We capture that on the website, news title and content have some analogues and the sentiments,like \"Sorry Ipads\". Considering that the tf-idf is based on the frequency to calculate between-document similarity and classification, while the word2vector is targetted to the words analogue in a semantic level, which corresponds to the context characteristic on the website, the word2vector finally is applied to process the context. To transform the text data, we first planned to create one pipeline to all the text columns, but it seems that the spark ml don't support it, so we build two pipelines to transform the title and context. Finally, the two vector models are saved to use predict the streaming news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30f23106-385e-4c9f-a55d-f3e397a0e14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 23:20:05 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# transform the test: using word2vector\n",
    "token1=Tokenizer(inputCol=\"title\", outputCol=\"words1\")\n",
    "token2=Tokenizer(inputCol=\"source_text\",outputCol=\"words2\")\n",
    "remover1=StopWordsRemover(inputCol=\"words1\", outputCol=\"removed1\")\n",
    "remover2=StopWordsRemover(inputCol=\"words2\",outputCol=\"removed2\")\n",
    "word2Vec1=Word2Vec(vectorSize=100, minCount=5, inputCol=\"removed1\", outputCol=\"word_vectors1\")\n",
    "word2Vec2=Word2Vec(vectorSize=100, minCount=5, inputCol=\"removed2\", outputCol=\"word_vectors2\")\n",
    "pipeline1=Pipeline(stages=[token1, remover1, word2Vec1])\n",
    "pipeline2=Pipeline(stages=[token2, remover2, word2Vec2])\n",
    "\n",
    "vecModel1=pipeline1.fit(train)\n",
    "train1=vecModel1.transform(train) \n",
    "test1=vecModel1.transform(test)\n",
    "\n",
    "vecModel2=pipeline2.fit(train1)\n",
    "train2=vecModel2.transform(train1)\n",
    "test2=vecModel2.transform(test1)\n",
    "\n",
    "assemble=VectorAssembler(inputCols=[\"word_vectors1\",\"word_vectors2\", \"votes\", \"comments\",\"hour\"], outputCol=\"features\")\n",
    "train_df=assemble.transform(train2)\n",
    "test_df=assemble.transform(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c00ad7d-7819-44c2-b14c-2f6305d1656b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 23:23:15 WARN TaskSetManager: Stage 51 contains a task of very large size (14445 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vecModel1.save(\"vecModel1\")\n",
    "vecModel2.save(\"vecModel2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2a5e5-8dae-40be-b9d3-8d6fb6fc08ff",
   "metadata": {},
   "source": [
    "<h2> 3.4 Model Fitting </h2>\n",
    "After processing the text, we assemble the features: title_vector, context_vector, comments, votes, and hour to fit the model. Four models are fitted to train the model: logistic regression, decision tree, random forest and gradient boost. All the results shows good performance. The random forest and gradient boost shows the sign of overfitting of the training dataset. The decision tree and the logistic regression both perform very well and similarly. However, the logistic regression shows a more robust performance considering the differences between train and test metrics and also among the metrics themselves. At the same time, logistics regression makes the intepretation easier. It captured the patterns and make it more intepretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74a8c875-c814-47e2-97c0-d01952dfb5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Model': LogisticRegression_7823cbb9385e,\n",
       "  'Test Evaluation': {'AUC': 0.9598289533773393,\n",
       "   'Accuracy': 0.9420600858369099,\n",
       "   'Precision': 0.9405149626869509,\n",
       "   'Recall': 0.9420600858369099,\n",
       "   'F1 Score': 0.9396963788008207},\n",
       "  'Train Evaluation': {'AUC': 0.9876079432274273,\n",
       "   'Accuracy': 0.9606741573033708,\n",
       "   'Precision': 0.959890203207267,\n",
       "   'Recall': 0.9606741573033708,\n",
       "   'F1 Score': 0.9597660614997663}},\n",
       " {'Model': DecisionTreeClassifier_86611e01980b,\n",
       "  'Test Evaluation': {'AUC': 0.9753061817577946,\n",
       "   'Accuracy': 0.9613733905579399,\n",
       "   'Precision': 0.9610285318872172,\n",
       "   'Recall': 0.9613733905579399,\n",
       "   'F1 Score': 0.9611705139228143},\n",
       "  'Train Evaluation': {'AUC': 0.9855081944358802,\n",
       "   'Accuracy': 0.969234884965222,\n",
       "   'Precision': 0.9698334439195779,\n",
       "   'Recall': 0.969234884965222,\n",
       "   'F1 Score': 0.9694710682797716}},\n",
       " {'Model': RandomForestClassifier_b0582e566a3c,\n",
       "  'Test Evaluation': {'AUC': 0.9583053099182139,\n",
       "   'Accuracy': 0.8615879828326181,\n",
       "   'Precision': 0.8812956541511525,\n",
       "   'Recall': 0.8615879828326181,\n",
       "   'F1 Score': 0.8175689238280163},\n",
       "  'Train Evaluation': {'AUC': 0.9706375654495871,\n",
       "   'Accuracy': 0.884430176565008,\n",
       "   'Precision': 0.8953481093691147,\n",
       "   'Recall': 0.884430176565008,\n",
       "   'F1 Score': 0.8559261033419714}},\n",
       " {'Model': GBTClassifier_02053fb84944,\n",
       "  'Test Evaluation': {'AUC': 0.9882841366712334,\n",
       "   'Accuracy': 0.9635193133047211,\n",
       "   'Precision': 0.9630788608537759,\n",
       "   'Recall': 0.9635193133047211,\n",
       "   'F1 Score': 0.9632294616948445},\n",
       "  'Train Evaluation': {'AUC': 0.998055480338984,\n",
       "   'Accuracy': 0.9791332263242376,\n",
       "   'Precision': 0.9792273647525778,\n",
       "   'Recall': 0.9791332263242375,\n",
       "   'F1 Score': 0.979175781513227}}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models={ \"Logistic Regression\": LogisticRegression(featuresCol=\"features\", labelCol=\"label\"),\n",
    "          \"Decision Tree\": DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\"), \n",
    "          \"Random Forest\": RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10),\n",
    "          \"Gradient Boost\": GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)}\n",
    "results=[]\n",
    "\n",
    "def evaluation(dataset):\n",
    "    bi_evaluator=BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "    auc=bi_evaluator.evaluate(dataset)\n",
    "    evaluator_accuracy=MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "    accuracy=evaluator_accuracy.evaluate(dataset)\n",
    "    evaluator_precision=MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"weightedPrecision\")\n",
    "    precision=evaluator_precision.evaluate(dataset)\n",
    "    evaluator_recall=MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"weightedRecall\")\n",
    "    recall=evaluator_recall.evaluate(dataset)\n",
    "    evaluator_f1=MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\")\n",
    "    f1_score=evaluator_f1.evaluate(dataset)\n",
    "    return {\"AUC\": auc,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1_score\n",
    "    }\n",
    "\n",
    "for model in models.values():\n",
    "    predictions=model.fit(train_df).transform(test_df)\n",
    "    train=model.fit(train_df).transform(train_df)\n",
    "    pred_eva=evaluation(predictions)\n",
    "    train_eva=evaluation(train)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model,\n",
    "        'Test Evaluation': pred_eva,\n",
    "        'Train Evaluation': train_eva\n",
    "    })\n",
    "    \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1afe2-1f82-4858-908f-125aa3ff1e00",
   "metadata": {},
   "source": [
    "From the results, we can see that the gradient boost actually has the best performance, but it could be prone to overfitting for the trainning data. Random forest shows a bigger difference between train dataset and test dataset, it could be also caused by the overfitting. The decision tree in the test dataset perform slightly worse than the logistic regression. Considering the precision and recall, also the intepretibility, the logistic regression is chosen to be the final model, saved for the further streaming prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b31ff065-d3ee-4002-9451-48267e5a4f10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_vec=LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_vec=model_vec.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9561e523-ce6b-424e-becd-e6d63e29575d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_vec.save(\"lr_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c3f23-630e-42cd-81f4-3859a8b2c275",
   "metadata": {},
   "source": [
    "<h2> 3.5 Streaming Prediction </h2>\n",
    "Finally we apply our logistic regression model to the streaming news to predict if the news will be on the frontpage, which shows a good prediction of the frontpage. To apply our model into the streaming news, we need to transform the feature that we did to the numeric features, and also the texts. By using the saved model of the word-to-vector model and logistic regression model, the texts are transformed to the vectors, and we finally get the results of the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a669612-c7c3-446b-8cd0-3aa7e2238209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.classification import LogisticRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b542d1cc-7201-4902-b2d8-c8ab9a5b2cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "832583eb-bfeb-49a1-9a79-87493f848c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "globals()['models_loaded'] = True\n",
    "globals()['my_model'] = LogisticRegressionModel.load(\"lr_model\")\n",
    "globals()['vecModel1'] = PipelineModel.load(\"vecModel1\")\n",
    "globals()['vecModel2'] = PipelineModel.load(\"vecModel2\")\n",
    "\n",
    "\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "\n",
    "    # transform the model to get relates features\n",
    "    df=df.withColumn(\"votes\", col(\"votes\").cast(IntegerType())) \\\n",
    "           .withColumn(\"comments\", col(\"comments\").cast(IntegerType())) \\\n",
    "           .withColumn(\"posted_at_ts\", to_timestamp(col(\"posted_at\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "           .withColumn(\"hour\", hour(col(\"posted_at_ts\")))\n",
    "    \n",
    "\n",
    "    if globals()['models_loaded']:\n",
    "        # Apply the pre-fitted pipelines\n",
    "        df1=globals()['vecModel1'].transform(df)\n",
    "        df2=globals()['vecModel2'].transform(df1)\n",
    "        \n",
    "        # Assemble features\n",
    "        assembler=VectorAssembler(inputCols=[\"word_vectors1\",\"word_vectors2\", \"votes\", \"comments\",\"hour\"], outputCol=\"features\")\n",
    "        df=assembler.transform(df2)\n",
    "    \n",
    "    df_result=globals()['my_model'].transform(df)\n",
    "    df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b503a7ed-7e21-48cd-8720-57aac2e2bbdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)\n",
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27d90e5e-65ea-47ff-807d-b7ec2d8276ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d40bb58-786e-453a-9f66-24091c1cbdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 23:38:29 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/23 23:38:29 WARN BlockManager: Block input-0-1716500309400 replicated to only 0 peer(s) instead of 1 peers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2024-05-23 23:38:30 =========\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+\n",
      "|     aid|comments|         domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|          user|votes|\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+\n",
      "|40452585|       0|clothes4all.net|    false|2024-05-23 09:41:05|Accessibility Ref...|Accessibility Ref...|Clothes4all â€“ An ...|https://www.cloth...|bryanrasmussen|    1|\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+-------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|         domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|          user|votes|       posted_at_ts|hour|              words1|            removed1|       word_vectors1|              words2|            removed2|       word_vectors2|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+-------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40452585|       0|clothes4all.net|    false|2024-05-23 09:41:05|Accessibility Ref...|Accessibility Ref...|Clothes4all â€“ An ...|https://www.cloth...|bryanrasmussen|    1|2024-05-23 09:41:05|   9|[clothes4all, â€“, ...|[clothes4all, â€“, ...|[-0.0159865278750...|[accessibility, r...|[accessibility, r...|[-0.0729413289750...|[-0.0159865278750...|[10.3285529820019...|[0.99996731471782...|       0.0|\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+-------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 23:38:32 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/23 23:38:32 WARN BlockManager: Block input-0-1716500312600 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/05/23 23:38:33 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/23 23:38:33 WARN BlockManager: Block input-0-1716500313600 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/05/23 23:38:39 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/23 23:38:39 WARN BlockManager: Block input-0-1716500318800 replicated to only 0 peer(s) instead of 1 peers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2024-05-23 23:38:40 =========\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+\n",
      "|     aid|comments|         domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|       user|votes|\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+\n",
      "|40452607|       0|      npmjs.com|    false|2024-05-23 09:45:09|dc-chat-widget - ...|      dc-chat-widget|Show HN: I built ...|https://www.npmjs...|NabilChiheb|    1|\n",
      "|40452611|       0|github.com/jawj|    false|2024-05-23 09:45:52|GitHub - jawj/mtw...|GitHub - jawj/mtw...|Mersenne Twister ...|https://github.co...|       gmac|    2|\n",
      "|40452625|       0|sourceforge.net|    false|2024-05-23 09:48:07|GrandPerspective\\...|    GrandPerspective|Grand Perspective...|https://grandpers...|   porsager|    1|\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+-------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|         domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|       user|votes|       posted_at_ts|hour|              words1|            removed1|       word_vectors1|              words2|            removed2|       word_vectors2|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+-------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40452607|       0|      npmjs.com|    false|2024-05-23 09:45:09|dc-chat-widget - ...|      dc-chat-widget|Show HN: I built ...|https://www.npmjs...|NabilChiheb|    1|2024-05-23 09:45:09|   9|[show, hn:, i, bu...|[show, hn:, built...|[-0.0439282485749...|[dc-chat-widget, ...|[dc-chat-widget, ...|[-0.0751537278222...|[-0.0439282485749...|[6.20952738207843...|[0.99799384545622...|       0.0|\n",
      "|40452611|       0|github.com/jawj|    false|2024-05-23 09:45:52|GitHub - jawj/mtw...|GitHub - jawj/mtw...|Mersenne Twister ...|https://github.co...|       gmac|    2|2024-05-23 09:45:52|   9|[mersenne, twiste...|[mersenne, twiste...|[4.9080781172961E...|[github, -, jawj/...|[github, -, jawj/...|[0.03438295401500...|[4.9080781172961E...|[4.34323028846363...|[0.98717220616671...|       0.0|\n",
      "|40452625|       0|sourceforge.net|    false|2024-05-23 09:48:07|GrandPerspective\\...|    GrandPerspective|Grand Perspective...|https://grandpers...|   porsager|    1|2024-05-23 09:48:07|   9|[grand, perspecti...|[grand, perspecti...|[-0.0110835232771...|[grandperspective...|[grandperspective...|[-0.0748832696852...|[-0.0110835232771...|[6.27086160639992...|[0.99811296869025...|       0.0|\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+-----+-------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 23:38:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/23 23:38:43 WARN BlockManager: Block input-0-1716500323600 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/05/23 23:38:47 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/23 23:38:47 WARN BlockManager: Block input-0-1716500326800 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/05/23 23:38:48 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/23 23:38:48 WARN BlockManager: Block input-0-1716500328600 replicated to only 0 peer(s) instead of 1 peers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2024-05-23 23:38:50 =========\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|        user|votes|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+\n",
      "|40452655|       5|tomwaitslibrary.info|     true|2024-05-23 09:52:18|Tom Waits - Frito...|         Frito-Lay -|Tom Waits vs. Fri...|http://tomwaitsli...|    Borrible|   25|\n",
      "|40452661|       0|    opensauced.pizza|    false|2024-05-23 09:52:36|OpenSauced Insigh...|OpenSauced Insigh...|Show HN: StarSear...|https://app.opens...| brianllamar|    1|\n",
      "|40452682|       0|           thehub.ca|    false|2024-05-23 09:55:58|Canada's military...|Canada's military...|Canada's military...|https://thehub.ca...|passwordoops|    1|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+-------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|        user|votes|       posted_at_ts|hour|              words1|            removed1|       word_vectors1|              words2|            removed2|       word_vectors2|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+-------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40452655|       5|tomwaitslibrary.info|     true|2024-05-23 09:52:18|Tom Waits - Frito...|         Frito-Lay -|Tom Waits vs. Fri...|http://tomwaitsli...|    Borrible|   25|2024-05-23 09:52:18|   9|[tom, waits, vs.,...|[tom, waits, vs.,...|[0.00246094372123...|[tom, waits, -, f...|[tom, waits, -, f...|[-0.0669265447401...|[0.00246094372123...|[-29.075203450308...|[2.35938916846255...|       1.0|\n",
      "|40452661|       0|    opensauced.pizza|    false|2024-05-23 09:52:36|OpenSauced Insigh...|OpenSauced Insigh...|Show HN: StarSear...|https://app.opens...| brianllamar|    1|2024-05-23 09:52:36|   9|[show, hn:, stars...|[show, hn:, stars...|[-0.0482860030606...|[opensauced, insi...|[opensauced, insi...|[-0.0453839403313...|[-0.0482860030606...|[14.1993012891539...|[0.99999931872648...|       0.0|\n",
      "|40452682|       0|           thehub.ca|    false|2024-05-23 09:55:58|Canada's military...|Canada's military...|Canada's military...|https://thehub.ca...|passwordoops|    1|2024-05-23 09:55:58|   9|[canada's, milita...|[canada's, milita...|[3.08931179461069...|[canada's, milita...|[canada's, milita...|[-0.0729512501479...|[3.08931179461069...|[4.94314590864839...|[0.99291838097343...|       0.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+-------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 23:38:53 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/23 23:38:53 WARN BlockManager: Block input-0-1716500333600 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/05/23 23:38:56 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/23 23:38:56 WARN BlockManager: Block input-0-1716500335800 replicated to only 0 peer(s) instead of 1 peers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2024-05-23 23:39:00 =========\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|         user|votes|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+\n",
      "|40452699|       0|medium.com/inthep...|    false|2024-05-23 09:57:47|Just a moment...\\...|    Just a moment...|Self-Serve Review...|https://medium.co...|dave_infuseai|    1|\n",
      "|40452698|       0|    cnx-software.com|    false|2024-05-23 09:57:47|ESP32-S3 1.69-inc...|ESP32-S3 1.69-inc...|ESP32-S3 1.69-inc...|https://www.cnx-s...|    PaulHoule|    1|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 23:39:00 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/23 23:39:00 WARN BlockManager: Block input-0-1716500340600 replicated to only 0 peer(s) instead of 1 peers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+-------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|         user|votes|       posted_at_ts|hour|              words1|            removed1|       word_vectors1|              words2|            removed2|       word_vectors2|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+-------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40452699|       0|medium.com/inthep...|    false|2024-05-23 09:57:47|Just a moment...\\...|    Just a moment...|Self-Serve Review...|https://medium.co...|dave_infuseai|    1|2024-05-23 09:57:47|   9|[self-serve, revi...|[self-serve, revi...|[-0.0014153423253...|[just, a, moment....|[moment..., , #, ...|[-0.0343874100705...|[-0.0014153423253...|[5.93990398246345...|[0.99737462833543...|       0.0|\n",
      "|40452698|       0|    cnx-software.com|    false|2024-05-23 09:57:47|ESP32-S3 1.69-inc...|ESP32-S3 1.69-inc...|ESP32-S3 1.69-inc...|https://www.cnx-s...|    PaulHoule|    1|2024-05-23 09:57:47|   9|[esp32-s3, 1.69-i...|[esp32-s3, 1.69-i...|[-2.0541255879733...|[esp32-s3, 1.69-i...|[esp32-s3, 1.69-i...|[-0.0742335910711...|[-2.0541255879733...|[5.83828017547321...|[0.99709461622238...|       0.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+-------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 23:39:02 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/23 23:39:02 WARN BlockManager: Block input-0-1716500342600 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/05/23 23:39:06 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/23 23:39:06 WARN BlockManager: Block input-0-1716500346200 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/05/23 23:39:07 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/23 23:39:07 WARN BlockManager: Block input-0-1716500346800 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/05/23 23:39:09 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/23 23:39:09 WARN BlockManager: Block input-0-1716500348800 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    }
   ],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b108f977-598c-4e88-a549-e10b44d45de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 23:39:09 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "24/05/23 23:39:09 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "24/05/23 23:39:09 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "24/05/23 23:39:09 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "Exception in thread \"receiver-supervisor-future-0\" java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.base/java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2024-05-23 23:39:10 =========\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|    user|votes|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+\n",
      "|40452701|       3|          ssleek.com|    false|2024-05-23 09:58:06|Welcome\\n\\nLOG IN...|             Welcome|Show HN Join Us i...|  https://ssleek.com|   zyx0r|    1|\n",
      "|40452704|       0|        chaos.social|     true|2024-05-23 09:58:17|chaos.social\\n\\nL...|        chaos.social|CompilerFax: Send...|https://chaos.soc...| signa11|    4|\n",
      "|40452711|       4|          reddit.com|     true|2024-05-23 09:58:57|Blocked\\n\\n# whoa...|             Blocked|        Is DDG Down?|https://old.reddi...|  tu7001|    7|\n",
      "|40452731|       0|  quantamagazine.org|     true|2024-05-23 10:03:27|How Failure Has M...|How Failure Has M...|How Failure Has M...|https://www.quant...|nsoonhui|    5|\n",
      "|40452738|       0|twitter.com/peter...|    false|2024-05-23 10:04:16|X\\n\\nDonâ€™t miss w...|                   X|Google AI partner...|https://twitter.c...|  nabla9|    1|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+\n",
      "\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+-------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|    user|votes|       posted_at_ts|hour|              words1|            removed1|       word_vectors1|              words2|            removed2|       word_vectors2|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+-------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40452701|       3|          ssleek.com|    false|2024-05-23 09:58:06|Welcome\\n\\nLOG IN...|             Welcome|Show HN Join Us i...|  https://ssleek.com|   zyx0r|    1|2024-05-23 09:58:06|   9|[show, hn, join, ...|[show, hn, join, ...|[-0.0293371671577...|[welcome, , log, ...|[welcome, , log, ...|[-0.1493757295935...|[-0.0293371671577...|[6.15502127567544...|[0.99788170089556...|       0.0|\n",
      "|40452704|       0|        chaos.social|     true|2024-05-23 09:58:17|chaos.social\\n\\nL...|        chaos.social|CompilerFax: Send...|https://chaos.soc...| signa11|    4|2024-05-23 09:58:17|   9|[compilerfax:, se...|[compilerfax:, se...|[-5.8885957696475...|[chaos.social, , ...|[chaos.social, , ...|[-0.1062674684716...|[-5.8885957696475...|[-1.4991857870104...|[0.18254699218673...|       1.0|\n",
      "|40452711|       4|          reddit.com|     true|2024-05-23 09:58:57|Blocked\\n\\n# whoa...|             Blocked|        Is DDG Down?|https://old.reddi...|  tu7001|    7|2024-05-23 09:58:57|   9|    [is, ddg, down?]|        [ddg, down?]|[0.0,0.0,0.0,0.0,...|[blocked, , #, wh...|[blocked, , #, wh...|[-0.1158775009933...|(203,[100,101,102...|[-3.4623845923091...|[0.03040166291159...|       1.0|\n",
      "|40452731|       0|  quantamagazine.org|     true|2024-05-23 10:03:27|How Failure Has M...|How Failure Has M...|How Failure Has M...|https://www.quant...|nsoonhui|    5|2024-05-23 10:03:27|  10|[how, failure, ha...|[failure, made, m...|[-0.0276946518570...|[how, failure, ha...|[failure, made, m...|[-0.0727521060737...|[-0.0276946518570...|[-0.3562967418213...|[0.41185631498751...|       1.0|\n",
      "|40452738|       0|twitter.com/peter...|    false|2024-05-23 10:04:16|X\\n\\nDonâ€™t miss w...|                   X|Google AI partner...|https://twitter.c...|  nabla9|    1|2024-05-23 10:04:16|  10|[google, ai, part...|[google, ai, part...|[-0.0011882593389...|[x, , donâ€™t, miss...|[x, , donâ€™t, miss...|[-0.1158245026562...|[-0.0011882593389...|[7.00506105595443...|[0.99909354386936...|       0.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+-------------------+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05ebca0-2fc4-40a9-ad91-67f80f474c2b",
   "metadata": {},
   "source": [
    "<h2> 3.6 Conclusion </h2>\n",
    "The result in fact shows a good performance on the frontpage, by using the source content and title as text features, votes, comments and published hour as numeric features. The text are vectorized to fit the model. But there are some improvements for the further study: first, the time moment is important to readers, because readers are more active during the morning, lunch and before-sleeping time as users behavior in daily life, we could weight the time to the features to get a percentile ordinal feature to evaluate the result. Second, we planned to use the domain as the reference, but there are many categories, it could be quantified as the frequency in the data collection to evaluate the importance, also we could rank the domain, but there should be more documents and sources to support it. Third, for the text process, we use word-to-vector to get the result of of semantic analysis, in fact, the tf-idf could also be utilized for the topic modeling. At first we calculate the tf-idf, but it shows a huge bulks of the features, which is time-consuming. But the topic and the semantic analysis could be adopted at the same time, if the computer could handle, which may extract more crucial information from the text for the predicition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
